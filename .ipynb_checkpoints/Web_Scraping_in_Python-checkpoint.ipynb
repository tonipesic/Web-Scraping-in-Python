{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Tree to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Intro HTML</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>Hello World!</p>\n",
    "    <p>Enjoy DataCamp!</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep it Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML code string\n",
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"class1\" id=\"div1\">\n",
    "      <p class=\"class2\">Visit DataCamp!</p>\n",
    "    </div>\n",
    "    <div class = \"you-are-classy\">\n",
    "      <p class=\"class2\">Keep up the good work!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where am I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Good Luck!</p>\n",
    "      <p>Not here...</p>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Where am I?</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '/html/body/div[2]/p[1]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Time to P\n",
    "Using double forward-slash notation, assign to the variable xpath a simple XPath string navigating to all paragraph p elements within any HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '//p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classy span\n",
    "Assign to the variable xpath an XPath string which will select all span elements whose class attribute equals \"span-class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blank\n",
    "xpath = '//span[@class=\"span-class\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPaths and Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body Appendages\n",
    "Assign to the variable xpath an XPath string which directs to all child elements of the body element. There is only one body element in this HTML document and it is a child of the root html element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XPath string to direct to children of body element\n",
    "xpath = '//body/*'\n",
    "\n",
    "# Print out the number of elements selected\n",
    "how_many_elements( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose DataCamp!\n",
    "Assign to the variable xpath an XPath string to direct to the paragraph element containing the phrase: \"Choose DataCamp!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Hello World!</p>\n",
    "      <div>\n",
    "        <p>Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XPath string to the desired paragraph element\n",
    "xpath = '/html/body/div[1]/div/p'\n",
    "\n",
    "# Print out the element text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where it's @\n",
    "Fill in the blanks in the XPath string to select the paragraph element containing the phrase: \"Thanks for Watching!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Xpath string to select desired p element\n",
    "xpath = '//*[@id=\"div3\"]/p'\n",
    "\n",
    "# Print out selection text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your Class\n",
    "Fill in the blanks in the xpath below to select the paragraph element containing the phrase: \"Hello World!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XPath string to select p element by class\n",
    "xpath = '//p[@class=\"class-1 class-2\"]'\n",
    "\n",
    "# Print out select text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper(link) Active\n",
    "Fill in the blanks to complete the variable xpath below to select the href attribute value from the DataCamp hyperlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose \n",
    "            <a href=\"http://datacamp.com\">DataCamp!</a>!\n",
    "        </p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xpath to the href attribute\n",
    "xpath = '//p[@id=\"p2\"]/a/@href'\n",
    "\n",
    "# Print out the selection(s); there should be only one\n",
    "print_attribute( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Links\n",
    "Fill in the blanks below to assign an XPath string to the variable xpath which directs to all href attribute values of the hyperlink a elements whose class attributes contain the string \"course-block\". Remember that we use the contains call within the XPath string to check if an attribute value contains a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xpath to the href attributes\n",
    "xpath = '//a[contains(@class,\"course-block\")]/@href'\n",
    "\n",
    "# Print out how many elements are selected\n",
    "how_many_elements( xpath )\n",
    "# Preview the selected elements\n",
    "preview( xpath )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath Chaining\n",
    "\n",
    "Fill in the blank below to chain together two xpath calls which result in the same selection as\n",
    "sel.xpath('//div/span/p[3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain together xpath methods to select desired p element\n",
    "sel.xpath( '//div' ).xpath('./span/p[3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divvy Up This Exercise\n",
    "Set up the Selector object sel with the html variable passed as the text argument.\n",
    "\n",
    "Assign to the variable divs a SelectorList of all div elements within the HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a Selector selecting html as the HTML document\n",
    "sel = Selector(html)\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath('.//*div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting a Selector\n",
    "Fill in the two blanks below to assign to create the Selector object sel which uses the string html as the text it inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1020 elements in the HTML document.\n",
      "You have found:  1020\n"
     ]
    }
   ],
   "source": [
    "url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "# Import a scrapy Selector\n",
    "from scrapy import Selector\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get( url ).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text = html)\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print( \"There are 1020 elements in the HTML document.\")\n",
    "print( \"You have found: \", len( sel.xpath('//*') ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Locators, Chaining, and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The (X)Path to CSS Locators\n",
    "Assign to the variable css_locator a CSS Locator string which is equivalent to the XPath string given.\n",
    "\n",
    "Assign to the variable xpath a XPath string which is equivalent to the CSS Locator string given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html > body > span:nth-of-type(1) a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an \"a\" in this Course\n",
    "Fill in the blank below to create the Selector object sel using the string html as the text input.\n",
    "\n",
    "Assign the variable css_locator a CSS Locator string which directs to the hyperlink (a element) children of all div elements belonging to the class \"course-block\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Fill in the blank\n",
    "css_locator = 'div.course-block > a'\n",
    "\n",
    "# Print the number of selected elements.\n",
    "how_many_elements( css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CSS Wildcard\n",
    "Assign to the variable css_locator a CSS Locator string which will select all children (regardless of tag-type) of the unique element in the HTML document that has its id attribute equal to uid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css_locator = '#uid > *'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You've been `href`ed\n",
    "Set up the Selector object sel using the string html as the text input.\n",
    "\n",
    "Assign to the variable hrefs_from_xpath the href attribute values from the elements in course_as. Your solution should match hrefs_from_css!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector(text = html )\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css( 'div.course-block > a' )\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css( '::attr(href)' )\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Level Text\n",
    "Assign to the variable xpath an XPath string directing to the text within the paragraph p element with id equal to p3, which does not include the text of future generations of this p element.\n",
    "\n",
    "Assign to the variable css_locator a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Level Text\n",
    "\n",
    "Assign to the variable xpath an XPath string directing to the text within the paragraph p element with id equal to p3, which includes the text of future generations of this p element.\n",
    "\n",
    "Assign to the variable css_locator a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]//text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3 ::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reveal By Response\n",
    "Assign to the variable this_url the URL used to load the response variable.\n",
    "\n",
    "Assign to the variable this_title the title of the website used to load the response variable. Since we only want the text from the single element we will select, we use the extract_first() method to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath( '/html/head/title/text()' ).extract_first()\n",
    "\n",
    "# Print out our findings\n",
    "print_url_title( this_url, this_title )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responding with Selectors\n",
    "Assign to the variable css_locator a CSS Locator string which directs to all hyperlink a elements belonging to the class course-block__link.\n",
    "\n",
    "Assign to the variable response_as the output of passing the css_locator variable to the css method in response.\n",
    "\n",
    "Assign to the variable sel_as the output of passing the css_locator variable to the css method in sel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSS Locator string to the desired hyperlink elements\n",
    "css_locator = 'a.course-block__link'\n",
    "\n",
    "# Select the hyperlink elements from response and sel\n",
    "response_as = response.css( css_locator )\n",
    "sel_as = sel.css( css_locator )\n",
    "\n",
    "# Examine similarity\n",
    "nr = len( response_as )\n",
    "ns = len( sel_as )\n",
    "for i in range( min(nr, ns, 2) ):\n",
    "  print( \"Element %d from response: %s\" % (i+1, response_as[i]) )\n",
    "  print( \"Element %d from sel: %s\" % (i+1, sel_as[i]) )\n",
    "  print( \"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting from a Selection\n",
    "Assign to the variable divs a SelectorList which selects all div elements belonging to the class course-block.\n",
    "\n",
    "Assign to the variable h4_text the text from the only h4 element within the content selected in first_div. Since we only want the text from the single element we will select, we use the extract_first() method to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all desired div elements\n",
    "divs = response.css( 'div.course-block' )\n",
    "\n",
    "# Take the first div element\n",
    "first_div = divs[0]\n",
    "\n",
    "# Extract the text from the h4 element in first_div\n",
    "h4_text = first_div.css('h4::text').extract_first()\n",
    "\n",
    "# Print out the text\n",
    "print( \"The text from the h4 element is:\", h4_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titular\n",
    "Using response, assign to the variable crs_title_els a SelectorList of the selected course titles.\n",
    "\n",
    "Assign to the variable crs_titles a list created by extracting the course titles from crs_title_els"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SelectorList of the course titles\n",
    "crs_title_els = response.css( 'h4::text' )\n",
    "\n",
    "# Extract the course titles \n",
    "crs_titles = crs_title_els.extract()\n",
    "\n",
    "# Print out the course titles \n",
    "for el in crs_titles:\n",
    "  print( \">>\", el )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping with Children\n",
    "Fill in the blank below to chain on a call to xpath so that we can calculate the number of children of the mystery element; we assign this number to the variable how_many_kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of children of the mystery element\n",
    "how_many_kids = len( mystery.xpath( './*' ) )\n",
    "\n",
    "# Print out the number\n",
    "print( \"The number of elements you selected was:\", how_many_kids )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheriting the Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class(c):\n",
    "    newc = c()\n",
    "    meths = dir(newc)\n",
    "    if 'name' in meths:\n",
    "        print(\"Your spider class name is:\", newc.name)\n",
    "    if 'from_crawler' in meths:\n",
    "        print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "    else:\n",
    "        print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = \"your_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        pass\n",
    "  # parse method\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurl the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    meths = dir( newc )\n",
    "    if 'start_requests' in meths:\n",
    "        print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "        print(  \"\\t-\", u )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start_requests method yields the following urls:\n",
      "\t- https://www.datacamp.com\n",
      "\t- https://scrapy.org\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "        for url in urls:\n",
    "            yield url\n",
    "  # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Referencing is Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    try:\n",
    "        newc.start_requests()\n",
    "    except:\n",
    "        print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling start_requests in YourSpider prints out: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = \"your_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        self.print_msg(\"Hello World!\")\n",
    "  # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "  # print_msg method\n",
    "    def print_msg( self, msg ):\n",
    "        print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Start Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    try:\n",
    "        y = list( newc.start_requests() )\n",
    "        first_yield = y[0]\n",
    "        print( \"The url you would scrape is:\", first_yield.url )\n",
    "        cb = first_yield.callback\n",
    "        print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "    except:\n",
    "        print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url you would scrape is: https://www.datacamp.com\n",
      "The name of the callback method you called is: parse\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "  # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pen Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req = list( news.start_requests() )[0]\n",
    "        url = req.url\n",
    "        html = requests.get( url ).content\n",
    "        response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "        author_names = req.callback( response )\n",
    "        print( 'You have collected the author names:')\n",
    "        for a in author_names:\n",
    "            print('\\t-', a )\n",
    "    except:\n",
    "        print( 'Oh no! Something went wrong with the code. Keep trying!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something went wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "    name = 'dcspider'\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "    def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "        author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "    # Here we will just return the list of Authors\n",
    "        return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req1 = list( news.start_requests() )[0]\n",
    "        html1 = requests.get( req1.url ).content\n",
    "        response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "        req2 = list( news.parse( response1 ) )[0]\n",
    "        html2 = requests.get( req2.url ).content\n",
    "        response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "        for d in news.parse_descr( response2 ):\n",
    "            print(\"One course description you found is:\", d )\n",
    "            break\n",
    "    except:\n",
    "        print(\"Oh no! Something is wrong with the code. Keep trying!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! Something is wrong with the code. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "    name = 'dcdescr'\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "    def parse( self, response ):\n",
    "        links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "        for link in links:\n",
    "            yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "  # Second parsing method\n",
    "    def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "        course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "        yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewCourses( dc_dict, n = 3 ):\n",
    "    crs_titles = list( dc_dict.keys() )\n",
    "    print( \"A preview of DataCamp Courses:\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    for t in crs_titles[:n]:\n",
    "        print( \"TITLE: %s\" % t)\n",
    "        for i,ct in enumerate(dc_dict[t]):\n",
    "            print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse_front)\n",
    "  # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url, callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCamp Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewCourses( dc_dict, n = 1 ):\n",
    "    crs_titles = list( dc_dict.keys() )\n",
    "    print( \"A preview of DataCamp Courses:\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    for t in crs_titles[:n]:\n",
    "        print( \"TITLE: %s\" % t)\n",
    "        print(\"\\tDescription: %s\" % dc_dict[t] )\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse_front)\n",
    "  # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url, callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css('p.course__description::text')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "        # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Crawler\n",
    "Assign to the variable crs_titles the extracted list of course titles on the DataCamp course directory page. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects.\n",
    "\n",
    "Assign to the variable crs_descrs the extracted list of short course descriptions. You should use the contains call within your XPath. You should use the contains call within your XPath, and your XPath string should point to the text of the selected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewCourses( dc_dict = dc_dict, n = 3 ):\n",
    "    parse( self = None, response = response )\n",
    "    crs_titles = list( dc_dict.keys() )\n",
    "    print( \"A preview of DataCamp Courses:\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    for t in crs_titles[:n]:\n",
    "        print( \"TITLE: %s\" % t)\n",
    "        print( \"\\tDESCRIPTION: %s\" % dc_dict[t] )\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = 'yourspider'\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "    def parse(self, response):\n",
    "        # My version of the parser you wrote in the previous part\n",
    "        crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "        crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "        for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "            dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
